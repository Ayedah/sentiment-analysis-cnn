Sentiment Analysis using Multi-Headed CNN

This project implements a multi-headed CNN for sentiment analysis of developer comments, leveraging word embeddings (GloVe + SO Vectors) and POS embeddings.

ğŸ“‚ Folder Structure

ğŸ“ sentiment-analysis-cnn
 â”œâ”€â”€ Create_matrices.py      # Prepares word and POS embeddings
 â”œâ”€â”€ NeuralNet.py            # Defines and trains the CNN model
 â”œâ”€â”€ Evaluate_model.py       # Evaluates the model on TS1 & TS2 datasets
 â”œâ”€â”€ tokenizer.pkl           # Saved tokenizer for consistent text processing
 â”œâ”€â”€ Multiheaded_CNN.json    # Saved model architecture
 â”œâ”€â”€ Multiheaded_CNN.h5      # Saved model weights
 â”œâ”€â”€ SavedData.h5            # Preprocessed embeddings and sequences
 â”œâ”€â”€ test_TS1.csv            # High-polarity test dataset
 â”œâ”€â”€ test_TS2.csv            # Low-polarity test dataset
 â”œâ”€â”€ evaluation_plots.png    # Precision-Recall & ROC curves
 â””â”€â”€ README.md               # Project documentation

ğŸ› ï¸ Setup Instructions

1ï¸âƒ£ Install Dependencies

pip install tensorflow keras pandas numpy gensim spacy h5py scikit-learn matplotlib

2ï¸âƒ£ Run Preprocessing

python Create_matrices.py

This script generates:

Tokenized sequences

Word embeddings (GloVe + SO Vectors fallback)

POS embeddings (One-hot encoding)

Saved data in SavedData.h5

3ï¸âƒ£ Train the Model

python NeuralNet.py

Trains the multi-headed CNN using word & POS embeddings

Saves trained model (Multiheaded_CNN.json & .h5)

4ï¸âƒ£ Evaluate the Model

python Evaluate_model.py

Loads test datasets (TS1 & TS2)

Tokenizes & extracts POS tags

Computes accuracy, precision, recall, F1-score, and confusion matrix

Plots Precision-Recall & ROC curves (evaluation_plots.png)

ğŸ“Š Model Details

Embeddings:

GloVe: General word embeddings

SO Vectors: SE domain-specific embeddings (fallback mechanism)

POS Embeddings: One-hot vectors based on spaCy POS tags

Model Architecture:

Two embedding inputs (word + POS)

Two 1D convolutional layers (Kernel size = 2, 100 filters each)

Batch normalization & ReLU activation

Global MaxPooling

Dense & dropout layers (0.25, 0.10)

Final output layer with Sigmoid activation

Optimizer: AdaMax

ğŸ“Œ Notes

Tokenizer consistency: Ensure tokenizer.pkl is used for all dataset splits

Data Leakage Avoidance: TS1 & TS2 were not used in embedding creation

Class Imbalance Handling: Training dataset used weighted classes
